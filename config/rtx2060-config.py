# ConfiguraÃ§Ã£o otimizada para RTX 2060 (6GB VRAM)

# ConfiguraÃ§Ãµes especÃ­ficas DeepSeek + RTX 2060
OLLAMA_RTX2060_CONFIG = {
    # Modelo otimizado para 6GB VRAM
    "recommended_model": "deepseek-coder:1.3b",  # Cabe perfeitamente na RTX 2060
    
    # ConfiguraÃ§Ãµes de GPU
    "gpu_layers": 35,           # Otimizado para RTX 2060
    "context_length": 2048,     # MÃ¡ximo para 6GB sem problemas
    "batch_size": 512,          # Equilibrio performance/memoria
    "threads": 8,               # RTX 2060 tem bom paralelismo
    
    # ConfiguraÃ§Ãµes de memÃ³ria
    "max_memory": 5500,         # Deixar ~500MB livres
    "memory_growth": True,      # Crescimento dinÃ¢mico
    "allow_growth": True,
    
    # Performance
    "flash_attention": True,
    "use_mlock": True,
    "use_mmap": True,
    "low_vram": False,          # RTX 2060 tem VRAM suficiente
    
    # ConcorrÃªncia (otimizada para local)
    "max_concurrent": 1,
    "num_parallel": 1,
    "max_queue": 128,
    
    # Estimativas de performance
    "expected_tokens_per_second": 25,  # ~25 tokens/s na RTX 2060
    "warmup_time": 3,                  # 3s para primeiro token
    "memory_usage_mb": 4800,           # ~4.8GB para DeepSeek 1.3B
}

print("ðŸŽ® ConfiguraÃ§Ã£o RTX 2060 carregada!")
print(f"ðŸ’¾ VRAM esperada: {OLLAMA_RTX2060_CONFIG['memory_usage_mb']}MB")
print(f"âš¡ Performance: ~{OLLAMA_RTX2060_CONFIG['expected_tokens_per_second']} tokens/s")
print(f"ðŸ§  Context: {OLLAMA_RTX2060_CONFIG['context_length']} tokens")
