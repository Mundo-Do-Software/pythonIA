# N8N + IAS - Sistema de InteligÃªncia Artificial Multi-Modelo

![License](https://img.shields.io/badge/license-MIT-blue.svg)
![Docker](https://img.shields.io/badge/docker-ready-green.svg)
![Python](https://img.shields.io/badge/python-3.10+-blue.svg)

## ğŸ—ï¸ Arquitetura do Projeto

Este projeto implementa um sistema completo de IA com **cache semÃ¢ntico**, suporte **multi-modelo** e integraÃ§Ã£o com **N8N** para automaÃ§Ã£o de workflows.

### ğŸ“‹ CaracterÃ­sticas Principais
- ğŸš€ **Cache SemÃ¢ntico Inteligente** - 874.8x speedup com Redis + embeddings
- ğŸ¤– **Multi-LLM Support** - Ollama, Mistral, Llama3.2
- âš¡ **API FastAPI** - Endpoints compatÃ­veis com OpenAI
- ğŸ”„ **N8N Integration** - AutomaÃ§Ã£o completa de workflows
- ğŸ³ **Docker Compose** - Deploy simplificado
- ğŸ§ª **Testing Suite** - Testes automatizados

```
N8N+IAS/
â”œâ”€â”€ ğŸ“‚ src/                     # CÃ³digo fonte principal
â”‚   â”œâ”€â”€ simple_llm_server.py    # Servidor principal da API
â”‚   â”œâ”€â”€ debug_models.py         # UtilitÃ¡rios de debug
â”‚   â””â”€â”€ llama_cpp_server_temp.py # Servidor alternativo
â”‚
â”œâ”€â”€ ğŸ“‚ config/                  # ConfiguraÃ§Ãµes
â”‚   â”œâ”€â”€ Dockerfile              # Container da API
â”‚   â””â”€â”€ .env                    # VariÃ¡veis de ambiente
â”‚
â”œâ”€â”€ ğŸ“‚ tests/                   # Testes organizados
â”‚   â”œâ”€â”€ unit/                   # Testes unitÃ¡rios
â”‚   â”œâ”€â”€ integration/            # Testes de integraÃ§Ã£o
â”‚   â””â”€â”€ performance/            # Testes de performance
â”‚
â”œâ”€â”€ ğŸ“‚ scripts/                 # Scripts de automaÃ§Ã£o
â”‚   â”œâ”€â”€ start.ps1               # Iniciar stack completa
â”‚   â”œâ”€â”€ stop.ps1                # Parar todos serviÃ§os
â”‚   â””â”€â”€ pull-model.ps1          # Baixar modelos Ollama
â”‚
â”œâ”€â”€ ğŸ“‚ docs/                    # DocumentaÃ§Ã£o completa
â”‚   â”œâ”€â”€ ENVIRONMENT_CONFIG.md   # ConfiguraÃ§Ã£o de variÃ¡veis
â”‚   â”œâ”€â”€ CLEANUP_SUMMARY.md      # HistÃ³rico de limpeza
â”‚   â””â”€â”€ README.md               # DocumentaÃ§Ã£o tÃ©cnica
â”‚
â”œâ”€â”€ ğŸ“‚ workflows/               # Workflows N8N (.json)
â”œâ”€â”€ ğŸ“‚ models/                  # Modelos LLM (*.gguf)
â”œâ”€â”€ ğŸ“‚ n8n_data/               # Dados persistentes N8N
â”œâ”€â”€ ğŸ“‚ loras/                  # Adaptadores LoRA
â”œâ”€â”€ ğŸ“„ docker-compose.yml       # OrquestraÃ§Ã£o principal
â”œâ”€â”€ ğŸ“„ run_tests.py             # Sistema de testes
â”œâ”€â”€ ğŸ“„ .gitignore               # Arquivos ignorados
â””â”€â”€ ğŸ“„ .dockerignore            # Build context otimizado
```

## ğŸš€ Quick Start

### 1. ConfiguraÃ§Ã£o Inicial
```powershell
# Clonar o repositÃ³rio
git clone <repo-url>
cd N8N+IAS

# Baixar modelo Ollama
./scripts/pull-model.ps1

# Configurar variÃ¡veis (editar se necessÃ¡rio)
# As configuraÃ§Ãµes estÃ£o em config/.env
```

### 2. Iniciar ServiÃ§os
```powershell
# Iniciar stack completa
./scripts/start.ps1

# Ou iniciar manualmente
docker-compose up -d

# Parar serviÃ§os
./scripts/stop.ps1
```

### 3. URLs de Acesso
- **LLM API**: http://localhost:5000
- **N8N**: http://localhost:5678 (admin/password123)  
- **Redis**: localhost:6379
- **Ollama**: http://localhost:11434

### 4. Verificar Sistema
```powershell
# Executar testes automatizados
python run_tests.py

# Testar categorias especÃ­ficas
python run_tests.py quick    # Testes rÃ¡pidos
python run_tests.py cache    # Cache semÃ¢ntico
python run_tests.py api      # API endpoints
python run_tests.py all      # Todos os testes

# Status dos containers
docker-compose ps
```

## ğŸ§ª Sistema de Testes

O projeto inclui uma **suite completa de testes** organizados em categorias:

### Testes Essenciais (`run_tests.py`)
- âœ… **API Super RÃ¡pida** - Conectividade bÃ¡sica
- âœ… **Teste RÃ¡pido Original** - Chat completions
- âœ… **CorreÃ§Ã£o Bug Timeout** - ValidaÃ§Ã£o do cache semÃ¢ntico

### Estrutura de Testes
```
tests/
â”œâ”€â”€ unit/              # Testes unitÃ¡rios isolados
â”œâ”€â”€ integration/       # Testes de integraÃ§Ã£o entre serviÃ§os  
â”œâ”€â”€ performance/       # Benchmarks e testes de carga
â”œâ”€â”€ test_super_quick.py    # Teste bÃ¡sico de conectividade
â”œâ”€â”€ test_quick.py          # Teste de chat completion
â””â”€â”€ test_bug_fix_final.py  # ValidaÃ§Ã£o do cache semÃ¢ntico
```

### Como Executar
```powershell
# Todos os testes essenciais
python run_tests.py

# ExecuÃ§Ã£o por categoria
python run_tests.py quick     # Apenas conectividade
python run_tests.py cache     # Cache + Redis
python run_tests.py api       # API completa
python run_tests.py help      # Ver opÃ§Ãµes
```
- `test_basic_api.py` - Testes bÃ¡sicos da API
- `test_simple_api.py` - Testes de funcionalidades simples
- `test_models.py` - Testes de seleÃ§Ã£o de modelos

### Testes de IntegraÃ§Ã£o (`tests/integration/`)
- `test_system_complete.py` - Testes completos do sistema
- `test_final_cache.py` - Testes do cache semÃ¢ntico
- `test_api.py` - Testes de integraÃ§Ã£o da API

### Testes de Performance (`tests/performance/`)
- `test_semantic_cache.py` - Performance do cache semÃ¢ntico
- `test_concurrency.py` - Testes de concorrÃªncia
- `test_velocidade.py` - Benchmarks de velocidade

## ğŸ“š Funcionalidades Principais

### âœ… Cache SemÃ¢ntico Inteligente
- **874.8x speedup** para perguntas similares
- Cache baseado em embeddings com similaridade > 85%
- ValidaÃ§Ã£o automÃ¡tica de respostas (nÃ£o cacheia erros/timeouts)
- TTL configurÃ¡vel (300s padrÃ£o)

### âœ… Multi-Modelo AutomÃ¡tico
- **SeleÃ§Ã£o inteligente** baseada no contexto
- Suporte a Mistral 7B e Llama 3.2 3B
- Fallback automÃ¡tico em caso de falha
- OtimizaÃ§Ã£o por tipo de consulta

### âœ… Infraestrutura Robusta
- **Docker Compose** para orquestraÃ§Ã£o
- **Redis** para cache persistente
- **GPU acceleration** com NVIDIA
- **Health checks** e monitoramento

### âœ… IntegraÃ§Ã£o N8N
- Workflows prontos para automaÃ§Ã£o
- Conectores para sistemas externos
- Processamento em lote

## ğŸ”§ ConfiguraÃ§Ã£o

### VariÃ¡veis de Ambiente
```env
# API Configuration
API_KEY=dfdjhasdfgldfugydlsuiflhgd
REDIS_URL=redis://redis:6379

# Ollama Configuration
OLLAMA_HOST=0.0.0.0
OLLAMA_GPU_LAYERS=999
OLLAMA_NUM_PARALLEL=2
OLLAMA_MAX_LOADED_MODELS=1
```

### Modelos Suportados
- **Mistral 7B** - Para consultas complexas e tÃ©cnicas
- **Llama 3.2 3B** - Para consultas simples e rÃ¡pidas
- **Auto-seleÃ§Ã£o** baseada em palavras-chave e tamanho

## ï¿½ï¸ Tecnologias Utilizadas

### Backend
- **Python 3.10+** - Linguagem principal
- **FastAPI** - Framework web assÃ­ncrono
- **Uvicorn** - Servidor ASGI de alta performance
- **Pydantic** - ValidaÃ§Ã£o de dados

### Cache & Database
- **Redis 7** - Cache semÃ¢ntico persistente
- **sentence-transformers** - Embeddings para similaridade
- **paraphrase-multilingual-MiniLM-L12-v2** - Modelo de embeddings

### LLM & AI
- **Ollama** - Runtime para modelos locais
- **Mistral 7B** - Modelo principal (4.4GB)
- **Llama 3.2 3B** - Modelo rÃ¡pido (2.0GB)

### Infrastructure
- **Docker Compose** - OrquestraÃ§Ã£o de containers
- **NVIDIA Docker** - AceleraÃ§Ã£o GPU
- **N8N** - AutomaÃ§Ã£o de workflows

### Monitoring & Testing
- **Sistema de testes personalizado** - ValidaÃ§Ã£o automatizada
- **Health checks** - Monitoramento de serviÃ§os
- **Logs estruturados** - Debug e troubleshooting

## ğŸ“Š Performance Benchmarks

| MÃ©trica | Cache Hit | Cache Miss | Speedup |
|---------|-----------|------------|---------|
| **Tempo Resposta** | 0.12-0.26s | 99.74s | **874.8x** |
| **Throughput** | ~4 req/s | ~0.01 req/s | **400x** |
| **Uso MemÃ³ria** | Baixo | Alto | **Otimizado** |
| **Uso GPU** | MÃ­nimo | Intensivo | **Eficiente** |

## ğŸ”§ ConfiguraÃ§Ã£o AvanÃ§ada

### VariÃ¡veis de Ambiente Principais
```env
# API Configuration
API_KEY=dfdjhasdfgldfugydlsuiflhgd    # Chave de autenticaÃ§Ã£o
API_HOST=0.0.0.0                     # Host da API
API_PORT=5000                        # Porta da API

# Cache Configuration  
REDIS_URL=redis://redis:6379          # URL do Redis
REDIS_MAX_MEMORY=512mb               # MemÃ³ria mÃ¡xima
REDIS_EVICTION_POLICY=allkeys-lru    # PolÃ­tica de remoÃ§Ã£o

# GPU Configuration
OLLAMA_GPU_LAYERS=999                # Usar todas camadas GPU
CUDA_VISIBLE_DEVICES=0               # GPU especÃ­fica
```

### Modelos DisponÃ­veis
- **ğŸš€ Mistral 7B** (`mistral:latest`) - 4.4GB
  - Melhor para: consultas complexas, anÃ¡lises tÃ©cnicas
  - Tempo mÃ©dio: 8-12s por resposta
  
- **âš¡ Llama 3.2 3B** (`llama3.2:3b`) - 2.0GB  
  - Melhor para: consultas simples, respostas rÃ¡pidas
  - Tempo mÃ©dio: 4-6s por resposta

### Cache SemÃ¢ntico
- **Threshold de Similaridade**: 85%
- **TTL (Time To Live)**: 300 segundos
- **Modelo de Embeddings**: paraphrase-multilingual-MiniLM-L12-v2
- **ValidaÃ§Ã£o**: NÃ£o cacheia erros ou timeouts

## ğŸ”— Endpoints da API

### POST `/v1/chat/completions`
Endpoint principal compatÃ­vel com OpenAI API
```json
{
  "model": "auto",
  "messages": [{"role": "user", "content": "Sua pergunta"}],
  "max_tokens": 100
}
```

### GET `/` 
Health check e informaÃ§Ãµes do sistema
```json
{
  "status": "running",
  "backend": "connected", 
  "model": "mistral",
  "api_version": "1.0.0"
}
```

## ğŸ› Troubleshooting

### Problemas Comuns

**âŒ API nÃ£o responde**
```powershell
# Verificar containers
docker-compose ps

# Ver logs
docker logs llm-api
docker logs llm-ollama

# Reiniciar serviÃ§os
docker-compose restart
```

**âŒ Cache nÃ£o funcionando**
```powershell
# Verificar Redis
docker logs llm-redis

# Testar conexÃ£o
python -c "import redis; r=redis.Redis(host='localhost', port=6379); print(r.ping())"
```

**âŒ Modelo nÃ£o encontrado**
```powershell
# Listar modelos
docker exec llm-ollama ollama list

# Baixar modelo
./scripts/pull-model.ps1
```

**âŒ Testes falhando**
```powershell
# Verificar serviÃ§os primeiro
python run_tests.py

# Executar teste especÃ­fico
python tests/test_super_quick.py
```

### Performance Issues

**ğŸŒ Respostas lentas**
- Verificar se GPU estÃ¡ sendo usada: `nvidia-smi`
- Conferir configuraÃ§Ã£o CUDA_VISIBLE_DEVICES
- Considerar usar modelo menor (Llama 3.2 3B)

**ğŸ’¾ Alto uso de memÃ³ria**
- Ajustar REDIS_MAX_MEMORY no `.env`
- Reduzir OLLAMA_MAX_LOADED_MODELS para 1
- Limpar cache: `docker exec llm-redis redis-cli FLUSHALL`

## ï¿½ Estrutura de Arquivos

### Arquivos Importantes
- `docker-compose.yml` - OrquestraÃ§Ã£o principal
- `config/.env` - VariÃ¡veis de ambiente
- `config/Dockerfile` - Build da API
- `src/simple_llm_server.py` - Servidor principal
- `run_tests.py` - Sistema de testes
- `.gitignore` / `.dockerignore` - Arquivos ignorados

### Dados Persistentes
- `models/` - Modelos Ollama (montado como volume)
- `n8n_data/` - Dados N8N (workflows, configuraÃ§Ãµes)
- `redis_data/` - Cache Redis (volume Docker)
- `ollama_data/` - Dados Ollama (volume Docker)

## ğŸ¤ ContribuiÃ§Ã£o

### Como Contribuir
1. **Fork** o repositÃ³rio
2. **Crie** uma branch para sua feature (`git checkout -b feature/nova-feature`)
3. **Commit** suas mudanÃ§as (`git commit -m 'Add: nova feature'`)
4. **Push** para a branch (`git push origin feature/nova-feature`)
5. **Abra** um Pull Request

### Desenvolvimento Local
```powershell
# Clone o projeto
git clone <repo-url>
cd N8N+IAS

# Configure ambiente
./scripts/pull-model.ps1

# Inicie desenvolvimento
docker-compose up -d

# Execute testes
python run_tests.py
```

## ğŸ“„ LicenÃ§a

Este projeto estÃ¡ sob a licenÃ§a **MIT**. Veja o arquivo `LICENSE` para mais detalhes.

## ğŸ¯ Roadmap

- [ ] **API v2** - Endpoints estendidos
- [ ] **Multi-GPU** - Suporte a mÃºltiplas GPUs  
- [ ] **Streaming** - Respostas em tempo real
- [ ] **RAG Integration** - Retrieval Augmented Generation
- [ ] **Web Interface** - Dashboard de monitoramento
- [ ] **Kubernetes** - Deploy em cluster

---

**Desenvolvido com â¤ï¸ para comunidade de IA**
  "model": "auto",
  "messages": [
    {"role": "user", "content": "Sua pergunta aqui"}
  ],
  "temperature": 0.7,
  "max_tokens": 512
}
```

### GET `/v1/models`
```json
{
  "data": [
    {"id": "mistral", "object": "model"},
    {"id": "llama3.2:3b", "object": "model"}
  ]
}
```

## ğŸ“– DocumentaÃ§Ã£o Detalhada

- [ğŸ“‹ Guia de ConfiguraÃ§Ã£o](docs/guides/QUICKSTART.md)
- [ğŸ”§ Setup do Postman](docs/guides/POSTMAN_GUIDE.md)
- [ğŸ—ï¸ Arquitetura do Sistema](docs/architecture/SISTEMA_FINAL_FUNCIONANDO.md)
- [ğŸ’° Exemplos Financeiros](docs/examples/FINANCIAL_KPI_EXAMPLES.md)
- [ğŸï¸ Caso de Uso - Motos](docs/examples/MODELO_RICARDO_MOTOS.md)

## ğŸ¤ Contribuindo

1. Fork o projeto
2. Crie uma branch: `git checkout -b feature/nova-funcionalidade`
3. Execute os testes: `python -m pytest tests/`
4. Commit: `git commit -m "Adiciona nova funcionalidade"`
5. Push: `git push origin feature/nova-funcionalidade`
6. Abra um Pull Request

## ğŸ› ï¸ Troubleshooting

### Problemas Comuns

1. **API nÃ£o responde**
   ```bash
   docker-compose -f docker-compose.ollama.yml logs llm-api
   ```

2. **Cache nÃ£o funciona**
   ```bash
   docker exec redis-cache redis-cli ping
   ```

3. **Modelo nÃ£o encontrado**
   ```bash
   docker exec ollama-server ollama list
   ```

## ğŸ“„ LicenÃ§a

Este projeto estÃ¡ sob a licenÃ§a MIT. Veja o arquivo `LICENSE` para mais detalhes.

---

**Desenvolvido com â¤ï¸ para automaÃ§Ã£o inteligente de processos**