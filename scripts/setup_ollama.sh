#!/bin/bash
# Script para configurar Ollama com modelo Mistral

echo "ğŸš€ Configurando Ollama com Mistral..."

# 1. Iniciar Ollama
echo "ğŸ“¦ Iniciando serviÃ§os Ollama..."
docker-compose -f docker-compose.ollama.yml up -d ollama

# Aguardar Ollama inicializar
echo "â³ Aguardando Ollama inicializar..."
sleep 10

# 2. Verificar se Ollama estÃ¡ rodando
echo "ğŸ” Verificando status do Ollama..."
curl -s http://localhost:11434/api/tags || echo "âŒ Ollama nÃ£o estÃ¡ respondendo"

# 3. Fazer pull do modelo Mistral
echo "ğŸ“¥ Baixando modelo Mistral (pode demorar)..."
curl -X POST http://localhost:11434/api/pull -H "Content-Type: application/json" -d '{"name": "mistral"}'

# 4. Verificar modelos disponÃ­veis
echo "ğŸ“‹ Modelos instalados:"
curl -s http://localhost:11434/api/tags | jq '.'

# 5. Iniciar servidor API
echo "ğŸ”¥ Iniciando servidor API..."
docker-compose -f docker-compose.ollama.yml up -d llm-api

# 6. Iniciar N8N
echo "ğŸ¯ Iniciando N8N..."
docker-compose -f docker-compose.ollama.yml up -d n8n

echo "âœ… Setup completo!"
echo "ğŸŒ Ollama: http://localhost:11434"
echo "ğŸ¤– API: http://localhost:5000"
echo "âš¡ N8N: http://localhost:5678 (admin/password123)"

# 7. Testar API
echo "ğŸ§ª Testando API..."
sleep 5
curl -X POST http://localhost:5000/v1/chat/completions \
  -H "Content-Type: application/json" \
  -d '{
    "model": "mistral",
    "messages": [
      {"role": "user", "content": "OlÃ¡! Como vocÃª estÃ¡?"}
    ],
    "temperature": 0.7,
    "max_tokens": 100
  }' | jq '.'
