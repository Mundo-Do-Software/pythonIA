{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "21128f1a",
   "metadata": {},
   "source": [
    "# üí∞ Boas Pr√°ticas para Vertex AI: Rodando DeepSeek de Forma Econ√¥mica\n",
    "\n",
    "Este notebook apresenta estrat√©gias pr√°ticas para reduzir custos ao rodar modelos LLM no Google Cloud Vertex AI, com foco nos modelos DeepSeek.\n",
    "\n",
    "## üéØ Principais Estrat√©gias de Economia:\n",
    "- **Spot Instances (Preempt√≠veis)**: At√© 70% de desconto\n",
    "- **Gerenciamento Autom√°tico**: Desligar quando n√£o usar\n",
    "- **Monitoramento de Custos**: Alertas em tempo real\n",
    "- **Cache Local**: Evitar downloads repetidos\n",
    "- **Otimiza√ß√£o CPU**: Formato GGML/GGUF para performance\n",
    "- **Controle de Concorr√™ncia**: Limitar requisi√ß√µes simult√¢neas\n",
    "\n",
    "## üìä Meta de Economia: Manter custos abaixo de R$ 300/m√™s"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b50e5a5",
   "metadata": {},
   "source": [
    "## üîß 1. Setup e Autentica√ß√£o\n",
    "\n",
    "Primeiro, vamos configurar as bibliotecas e autentica√ß√£o necess√°rias."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61bd11df",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instalar depend√™ncias necess√°rias\n",
    "!pip install google-cloud-aiplatform google-cloud-billing google-cloud-compute google-auth --quiet\n",
    "\n",
    "# Imports necess√°rios\n",
    "import os\n",
    "import json\n",
    "import time\n",
    "import asyncio\n",
    "from datetime import datetime, timedelta\n",
    "from google.cloud import aiplatform\n",
    "from google.cloud import billing_v1\n",
    "from google.cloud import compute_v1\n",
    "from google.auth import default\n",
    "import requests\n",
    "\n",
    "# Configura√ß√µes do projeto\n",
    "PROJECT_ID = \"seu-projeto-gcp\"  # ‚ö†Ô∏è ALTERE AQUI\n",
    "REGION = \"us-central1\"\n",
    "ZONE = \"us-central1-a\"\n",
    "\n",
    "# Verificar autentica√ß√£o\n",
    "try:\n",
    "    credentials, project = default()\n",
    "    print(f\"‚úÖ Autenticado no projeto: {project}\")\n",
    "    if project != PROJECT_ID:\n",
    "        print(f\"‚ö†Ô∏è Projeto configurado: {PROJECT_ID}, mas autenticado em: {project}\")\n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Erro na autentica√ß√£o: {e}\")\n",
    "    print(\"üí° Execute: gcloud auth application-default login\")\n",
    "\n",
    "# Inicializar Vertex AI\n",
    "aiplatform.init(project=PROJECT_ID, location=REGION)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13c3b582",
   "metadata": {},
   "source": [
    "## üéØ 2. Configurando Spot Instances (At√© 70% Economia!)\n",
    "\n",
    "Spot instances (preempt√≠veis) s√£o a melhor forma de economizar. Perfeitas para infer√™ncia de IA."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "520e1f29",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fun√ß√£o para criar Custom Job com Spot Instances\n",
    "def create_spot_training_job(\n",
    "    display_name: str = \"deepseek-spot-inference\",\n",
    "    machine_type: str = \"n1-standard-4\",\n",
    "    accelerator_type: str = \"NVIDIA_TESLA_T4\",\n",
    "    accelerator_count: int = 1\n",
    "):\n",
    "    \"\"\"Cria um Custom Job usando spot instances para economia.\"\"\"\n",
    "    \n",
    "    # Configura√ß√£o do worker pool com spot instances\n",
    "    worker_pool_specs = [\n",
    "        {\n",
    "            \"machine_spec\": {\n",
    "                \"machine_type\": machine_type,\n",
    "                \"accelerator_type\": accelerator_type,\n",
    "                \"accelerator_count\": accelerator_count,\n",
    "            },\n",
    "            \"replica_count\": 1,\n",
    "            \"container_spec\": {\n",
    "                \"image_uri\": \"ollama/ollama:latest\",\n",
    "                \"command\": [\"ollama\", \"serve\"],\n",
    "                \"env\": [\n",
    "                    {\"name\": \"OLLAMA_HOST\", \"value\": \"0.0.0.0\"},\n",
    "                    {\"name\": \"OLLAMA_MAX_LOADED_MODELS\", \"value\": \"1\"},  # Economia de mem√≥ria\n",
    "                    {\"name\": \"OLLAMA_NUM_PARALLEL\", \"value\": \"1\"},       # Menor concorr√™ncia\n",
    "                ]\n",
    "            },\n",
    "            # üéØ CHAVE: Habilitar spot instances (preempt√≠vel)\n",
    "            \"disk_spec\": {\n",
    "                \"boot_disk_type\": \"pd-ssd\",\n",
    "                \"boot_disk_size_gb\": 100\n",
    "            },\n",
    "        }\n",
    "    ]\n",
    "    \n",
    "    job_spec = {\n",
    "        \"display_name\": display_name,\n",
    "        \"job_spec\": {\n",
    "            \"worker_pool_specs\": worker_pool_specs,\n",
    "            \"scheduling\": {\n",
    "                \"restart_job_on_worker_restart\": True,\n",
    "                \"timeout\": \"7200s\"  # 2 horas max\n",
    "            },\n",
    "            # üí∞ ECONOMIA: Usar spot instances\n",
    "            \"service_account\": f\"vertex-ai@{PROJECT_ID}.iam.gserviceaccount.com\"\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    return job_spec\n",
    "\n",
    "# Exemplo de uso\n",
    "spot_job_config = create_spot_training_job()\n",
    "print(\"üéØ Configura√ß√£o de Spot Instance criada!\")\n",
    "print(f\"üí∞ Economia esperada: at√© 70% comparado a inst√¢ncias normais\")\n",
    "print(json.dumps(spot_job_config, indent=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88462d27",
   "metadata": {},
   "source": [
    "## ‚è∞ 3. Gerenciamento Autom√°tico de Inst√¢ncias\n",
    "\n",
    "Scripts para automatizar ligar/desligar inst√¢ncias e evitar custos desnecess√°rios."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64ab2f84",
   "metadata": {},
   "outputs": [],
   "source": [
    "import subprocess\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "class InstanceManager:\n",
    "    \"\"\"Gerenciador autom√°tico de inst√¢ncias para economia.\"\"\"\n",
    "    \n",
    "    def __init__(self, project_id: str, zone: str):\n",
    "        self.project_id = project_id\n",
    "        self.zone = zone\n",
    "        self.compute_client = compute_v1.InstancesClient()\n",
    "    \n",
    "    def stop_instance_if_idle(self, instance_name: str, max_idle_minutes: int = 30):\n",
    "        \"\"\"Para inst√¢ncia se estiver ociosa por mais de X minutos.\"\"\"\n",
    "        try:\n",
    "            # Verificar se h√° atividade recente\n",
    "            request = compute_v1.GetInstanceRequest(\n",
    "                project=self.project_id,\n",
    "                zone=self.zone,\n",
    "                instance=instance_name\n",
    "            )\n",
    "            instance = self.compute_client.get(request=request)\n",
    "            \n",
    "            # Verificar se est√° rodando\n",
    "            if instance.status != \"RUNNING\":\n",
    "                print(f\"üî¥ Inst√¢ncia {instance_name} j√° est√° parada\")\n",
    "                return\n",
    "            \n",
    "            # Aqui voc√™ pode adicionar l√≥gica para verificar CPU/GPU usage\n",
    "            # Por exemplo, usando Cloud Monitoring API\n",
    "            \n",
    "            print(f\"‚èπÔ∏è Parando inst√¢ncia {instance_name} para economizar\")\n",
    "            stop_request = compute_v1.StopInstanceRequest(\n",
    "                project=self.project_id,\n",
    "                zone=self.zone,\n",
    "                instance=instance_name\n",
    "            )\n",
    "            operation = self.compute_client.stop(request=stop_request)\n",
    "            print(f\"‚úÖ Opera√ß√£o iniciada: {operation.name}\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå Erro ao parar inst√¢ncia: {e}\")\n",
    "    \n",
    "    def start_instance_if_needed(self, instance_name: str):\n",
    "        \"\"\"Inicia inst√¢ncia se ela estiver parada.\"\"\"\n",
    "        try:\n",
    "            request = compute_v1.GetInstanceRequest(\n",
    "                project=self.project_id,\n",
    "                zone=self.zone,\n",
    "                instance=instance_name\n",
    "            )\n",
    "            instance = self.compute_client.get(request=request)\n",
    "            \n",
    "            if instance.status == \"RUNNING\":\n",
    "                print(f\"‚úÖ Inst√¢ncia {instance_name} j√° est√° rodando\")\n",
    "                return\n",
    "            \n",
    "            print(f\"üü¢ Iniciando inst√¢ncia {instance_name}\")\n",
    "            start_request = compute_v1.StartInstanceRequest(\n",
    "                project=self.project_id,\n",
    "                zone=self.zone,\n",
    "                instance=instance_name\n",
    "            )\n",
    "            operation = self.compute_client.start(request=start_request)\n",
    "            print(f\"‚úÖ Opera√ß√£o iniciada: {operation.name}\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå Erro ao iniciar inst√¢ncia: {e}\")\n",
    "    \n",
    "    def schedule_auto_stop(self, instance_name: str, stop_time: str = \"22:00\"):\n",
    "        \"\"\"Agenda parada autom√°tica da inst√¢ncia.\"\"\"\n",
    "        print(f\"üìÖ Agendando parada autom√°tica de {instance_name} √†s {stop_time}\")\n",
    "        \n",
    "        # Exemplo usando crontab (Linux) ou Task Scheduler (Windows)\n",
    "        cron_command = f\"0 22 * * * gcloud compute instances stop {instance_name} --zone={self.zone} --project={self.project_id}\"\n",
    "        print(f\"üîß Comando cron: {cron_command}\")\n",
    "        return cron_command\n",
    "\n",
    "# Exemplo de uso\n",
    "manager = InstanceManager(PROJECT_ID, ZONE)\n",
    "\n",
    "# Simular gerenciamento\n",
    "instance_name = \"ollama-deepseek-vm\"\n",
    "print(\"üöÄ Gerenciador de inst√¢ncias iniciado\")\n",
    "print(\"üí° Use este c√≥digo em um script agendado para economia autom√°tica\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b014bc0",
   "metadata": {},
   "source": [
    "## üìä 4. Monitoramento de Custos e Alertas\n",
    "\n",
    "Configure alertas para n√£o queimar o or√ßamento de R$ 1.900!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9f11d44",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CostMonitor:\n",
    "    \"\"\"Monitor de custos para alertas em tempo real.\"\"\"\n",
    "    \n",
    "    def __init__(self, project_id: str):\n",
    "        self.project_id = project_id\n",
    "        self.billing_client = billing_v1.CloudBillingClient()\n",
    "    \n",
    "    def get_current_month_cost(self):\n",
    "        \"\"\"Obt√©m custo do m√™s atual.\"\"\"\n",
    "        try:\n",
    "            # Este √© um exemplo simplificado\n",
    "            # Na pr√°tica, voc√™ usaria a Billing API para dados reais\n",
    "            \n",
    "            from datetime import datetime\n",
    "            now = datetime.now()\n",
    "            month_start = now.replace(day=1, hour=0, minute=0, second=0, microsecond=0)\n",
    "            \n",
    "            # Simula√ß√£o de custo (substitua pela API real)\n",
    "            estimated_cost_usd = 85.50  # Exemplo\n",
    "            estimated_cost_brl = estimated_cost_usd * 5.2  # Convers√£o aproximada\n",
    "            \n",
    "            return {\n",
    "                \"cost_usd\": estimated_cost_usd,\n",
    "                \"cost_brl\": estimated_cost_brl,\n",
    "                \"period\": f\"{month_start.strftime('%Y-%m')}\"\n",
    "            }\n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå Erro ao obter custos: {e}\")\n",
    "            return None\n",
    "    \n",
    "    def check_budget_alert(self, max_budget_brl: float = 300.0):\n",
    "        \"\"\"Verifica se est√° pr√≥ximo do or√ßamento.\"\"\"\n",
    "        cost_data = self.get_current_month_cost()\n",
    "        \n",
    "        if not cost_data:\n",
    "            return\n",
    "            \n",
    "        current_cost = cost_data[\"cost_brl\"]\n",
    "        percentage = (current_cost / max_budget_brl) * 100\n",
    "        \n",
    "        print(f\"üí∞ Custo atual: R$ {current_cost:.2f}\")\n",
    "        print(f\"üéØ Or√ßamento: R$ {max_budget_brl:.2f}\")\n",
    "        print(f\"üìä Utilizado: {percentage:.1f}%\")\n",
    "        \n",
    "        if percentage >= 90:\n",
    "            print(\"üö® ALERTA CR√çTICO: 90%+ do or√ßamento utilizado!\")\n",
    "            return \"CRITICAL\"\n",
    "        elif percentage >= 70:\n",
    "            print(\"‚ö†Ô∏è ALERTA: 70%+ do or√ßamento utilizado!\")\n",
    "            return \"WARNING\"\n",
    "        elif percentage >= 50:\n",
    "            print(\"üì¢ ATEN√á√ÉO: 50%+ do or√ßamento utilizado!\")\n",
    "            return \"INFO\"\n",
    "        else:\n",
    "            print(\"‚úÖ Or√ßamento dentro do planejado\")\n",
    "            return \"OK\"\n",
    "    \n",
    "    def create_budget_alert_webhook(self, webhook_url: str, budget_brl: float):\n",
    "        \"\"\"Cria webhook para alertas de or√ßamento.\"\"\"\n",
    "        \n",
    "        import requests\n",
    "        \n",
    "        def send_alert(message: str, level: str):\n",
    "            payload = {\n",
    "                \"text\": f\"üö® GCP Budget Alert: {message}\",\n",
    "                \"level\": level,\n",
    "                \"project\": self.project_id,\n",
    "                \"timestamp\": datetime.now().isoformat()\n",
    "            }\n",
    "            \n",
    "            try:\n",
    "                response = requests.post(webhook_url, json=payload, timeout=5)\n",
    "                print(f\"üì® Alerta enviado: {response.status_code}\")\n",
    "            except Exception as e:\n",
    "                print(f\"‚ùå Erro ao enviar alerta: {e}\")\n",
    "        \n",
    "        return send_alert\n",
    "\n",
    "# Monitoramento de custos\n",
    "monitor = CostMonitor(PROJECT_ID)\n",
    "\n",
    "# Verificar or√ßamento atual\n",
    "alert_level = monitor.check_budget_alert(max_budget_brl=300.0)\n",
    "\n",
    "# Dicas de economia baseadas no n√≠vel de alerta\n",
    "if alert_level in [\"WARNING\", \"CRITICAL\"]:\n",
    "    print(\"\\nüîß DICAS DE ECONOMIA IMEDIATA:\")\n",
    "    print(\"1. Pause inst√¢ncias n√£o utilizadas\")\n",
    "    print(\"2. Use apenas DeepSeek 1.3B para testes\")\n",
    "    print(\"3. Ative modo spot em todas as inst√¢ncias\")\n",
    "    print(\"4. Reduza max_concurrent_requests para 1\")\n",
    "    print(\"5. Configure auto-stop √†s 18:00\")\n",
    "\n",
    "# URL para dashboard de custos\n",
    "print(f\"\\nüåê Dashboard de custos: https://console.cloud.google.com/billing/projects/{PROJECT_ID}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2db7205",
   "metadata": {},
   "source": [
    "## üíæ 5. Armazenamento Local e Cache de Modelos\n",
    "\n",
    "Evite downloads repetidos e economize banda!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f1394f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil\n",
    "from pathlib import Path\n",
    "\n",
    "class ModelCacheManager:\n",
    "    \"\"\"Gerenciador de cache local para modelos.\"\"\"\n",
    "    \n",
    "    def __init__(self, cache_dir: str = \"/opt/models\"):\n",
    "        self.cache_dir = Path(cache_dir)\n",
    "        self.cache_dir.mkdir(parents=True, exist_ok=True)\n",
    "        \n",
    "    def is_model_cached(self, model_name: str) -> bool:\n",
    "        \"\"\"Verifica se o modelo j√° est√° no cache local.\"\"\"\n",
    "        model_path = self.cache_dir / model_name\n",
    "        return model_path.exists() and model_path.stat().st_size > 0\n",
    "    \n",
    "    def get_model_size(self, model_name: str) -> int:\n",
    "        \"\"\"Obt√©m tamanho do modelo em bytes.\"\"\"\n",
    "        model_path = self.cache_dir / model_name\n",
    "        if model_path.exists():\n",
    "            return model_path.stat().st_size\n",
    "        return 0\n",
    "    \n",
    "    def cache_model_locally(self, model_name: str, source_url: str = None):\n",
    "        \"\"\"Baixa e armazena modelo localmente.\"\"\"\n",
    "        model_path = self.cache_dir / model_name\n",
    "        \n",
    "        if self.is_model_cached(model_name):\n",
    "            size_mb = self.get_model_size(model_name) / (1024 * 1024)\n",
    "            print(f\"‚úÖ Modelo {model_name} j√° est√° no cache ({size_mb:.1f} MB)\")\n",
    "            return str(model_path)\n",
    "        \n",
    "        print(f\"üì• Baixando {model_name} para cache local...\")\n",
    "        \n",
    "        # Para Ollama, usar o comando pull para cache local\n",
    "        if \"deepseek\" in model_name.lower():\n",
    "            os.system(f\"ollama pull {model_name}\")\n",
    "            print(f\"‚úÖ {model_name} armazenado no cache do Ollama\")\n",
    "        \n",
    "        return str(model_path)\n",
    "    \n",
    "    def cleanup_old_models(self, keep_latest: int = 2):\n",
    "        \"\"\"Remove modelos antigos para economizar espa√ßo.\"\"\"\n",
    "        model_files = list(self.cache_dir.glob(\"*\"))\n",
    "        model_files.sort(key=lambda x: x.stat().st_mtime, reverse=True)\n",
    "        \n",
    "        if len(model_files) > keep_latest:\n",
    "            for old_model in model_files[keep_latest:]:\n",
    "                print(f\"üóëÔ∏è Removendo modelo antigo: {old_model.name}\")\n",
    "                if old_model.is_file():\n",
    "                    old_model.unlink()\n",
    "                elif old_model.is_dir():\n",
    "                    shutil.rmtree(old_model)\n",
    "    \n",
    "    def get_cache_stats(self):\n",
    "        \"\"\"Estat√≠sticas do cache.\"\"\"\n",
    "        total_size = sum(f.stat().st_size for f in self.cache_dir.rglob('*') if f.is_file())\n",
    "        model_count = len(list(self.cache_dir.iterdir()))\n",
    "        \n",
    "        return {\n",
    "            \"total_size_gb\": total_size / (1024**3),\n",
    "            \"model_count\": model_count,\n",
    "            \"cache_dir\": str(self.cache_dir)\n",
    "        }\n",
    "\n",
    "# Script de inicializa√ß√£o do cache\n",
    "def setup_persistent_storage():\n",
    "    \"\"\"Configura armazenamento persistente para modelos.\"\"\"\n",
    "    \n",
    "    # Criar script de inicializa√ß√£o\n",
    "    init_script = '''#!/bin/bash\n",
    "# Script de inicializa√ß√£o para cache de modelos\n",
    "set -e\n",
    "\n",
    "echo \"üöÄ Configurando cache persistente de modelos...\"\n",
    "\n",
    "# Criar diret√≥rio de cache\n",
    "mkdir -p /opt/models\n",
    "mkdir -p /root/.ollama\n",
    "\n",
    "# Se h√° backup no Google Cloud Storage, restaurar\n",
    "if gsutil ls gs://seu-bucket-models/backup/ 2>/dev/null; then\n",
    "    echo \"üì¶ Restaurando modelos do backup...\"\n",
    "    gsutil -m cp -r gs://seu-bucket-models/backup/* /root/.ollama/\n",
    "fi\n",
    "\n",
    "# Pr√©-carregar modelos essenciais\n",
    "echo \"üì• Pr√©-carregando DeepSeek 1.3B...\"\n",
    "ollama pull deepseek-coder:1.3b\n",
    "\n",
    "# Backup peri√≥dico (opcional)\n",
    "cat > /opt/backup_models.sh << 'EOF'\n",
    "#!/bin/bash\n",
    "echo \"üíæ Fazendo backup dos modelos...\"\n",
    "gsutil -m cp -r /root/.ollama gs://seu-bucket-models/backup/\n",
    "echo \"‚úÖ Backup conclu√≠do\"\n",
    "EOF\n",
    "\n",
    "chmod +x /opt/backup_models.sh\n",
    "\n",
    "echo \"‚úÖ Cache configurado com sucesso!\"\n",
    "'''\n",
    "    \n",
    "    with open(\"/tmp/init_models.sh\", \"w\") as f:\n",
    "        f.write(init_script)\n",
    "    \n",
    "    print(\"üìù Script de inicializa√ß√£o criado em /tmp/init_models.sh\")\n",
    "    print(\"üîß Execute com: chmod +x /tmp/init_models.sh && ./tmp/init_models.sh\")\n",
    "\n",
    "# Exemplo de uso\n",
    "cache_manager = ModelCacheManager()\n",
    "\n",
    "# Verificar modelos no cache\n",
    "stats = cache_manager.get_cache_stats()\n",
    "print(f\"üìä Cache atual: {stats['model_count']} modelos, {stats['total_size_gb']:.2f} GB\")\n",
    "\n",
    "# Configurar script de inicializa√ß√£o\n",
    "setup_persistent_storage()\n",
    "\n",
    "print(\"\\nüí° DICAS DE ECONOMIA:\")\n",
    "print(\"1. Use PVC (Persistent Volume) para modelos\")\n",
    "print(\"2. Configure backup incremental no GCS\")\n",
    "print(\"3. Limite cache a 2-3 modelos mais usados\")\n",
    "print(\"4. Use compress√£o para reduzir tamanho\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0681879d",
   "metadata": {},
   "source": [
    "## üöÄ 6. Otimiza√ß√£o com GGML/GGUF (CPU Performance)\n",
    "\n",
    "Use formato GGML para rodar em CPU com performance otimizada!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a244a621",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configura√ß√£o para usar CPU otimizada com GGML\n",
    "def create_cpu_optimized_deployment():\n",
    "    \"\"\"Cria deployment otimizado para CPU com GGML.\"\"\"\n",
    "    \n",
    "    dockerfile_content = '''\n",
    "FROM ubuntu:22.04\n",
    "\n",
    "# Instalar depend√™ncias\n",
    "RUN apt-get update && apt-get install -y \\\\\n",
    "    build-essential \\\\\n",
    "    cmake \\\\\n",
    "    git \\\\\n",
    "    python3 \\\\\n",
    "    python3-pip \\\\\n",
    "    curl \\\\\n",
    "    && rm -rf /var/lib/apt/lists/*\n",
    "\n",
    "# Instalar llama.cpp para GGML\n",
    "WORKDIR /opt\n",
    "RUN git clone https://github.com/ggerganov/llama.cpp.git\n",
    "WORKDIR /opt/llama.cpp\n",
    "\n",
    "# Compilar com otimiza√ß√µes AVX2\n",
    "RUN make LLAMA_OPENBLAS=1 LLAMA_AVX2=1 -j4\n",
    "\n",
    "# Instalar Python bindings\n",
    "RUN pip3 install llama-cpp-python\n",
    "\n",
    "# Script de servidor\n",
    "COPY server.py /opt/server.py\n",
    "\n",
    "# Vari√°veis de ambiente para otimiza√ß√£o\n",
    "ENV OMP_NUM_THREADS=4\n",
    "ENV LLAMA_CPP_PARALLEL=1\n",
    "ENV LLAMA_CPP_BATCH_SIZE=512\n",
    "\n",
    "EXPOSE 8000\n",
    "CMD [\"python3\", \"/opt/server.py\"]\n",
    "'''\n",
    "    \n",
    "    server_code = '''\n",
    "from llama_cpp import Llama\n",
    "from fastapi import FastAPI, HTTPException\n",
    "import asyncio\n",
    "import uvicorn\n",
    "\n",
    "app = FastAPI()\n",
    "\n",
    "# Carregar modelo GGUF otimizado\n",
    "llm = Llama(\n",
    "    model_path=\"/opt/models/deepseek-coder-1.3b.gguf\",\n",
    "    n_ctx=2048,        # Contexto menor = menos mem√≥ria\n",
    "    n_batch=512,       # Batch size otimizado\n",
    "    n_threads=4,       # Threads da CPU\n",
    "    verbose=False,\n",
    "    use_mlock=True,    # Lock na mem√≥ria para performance\n",
    "    use_mmap=True,     # Memory mapping\n",
    "    low_vram=True      # Otimizar para pouca VRAM\n",
    ")\n",
    "\n",
    "@app.post(\"/v1/chat/completions\")\n",
    "async def chat_completions(request: dict):\n",
    "    try:\n",
    "        messages = request.get(\"messages\", [])\n",
    "        prompt = messages[-1][\"content\"] if messages else \"\"\n",
    "        \n",
    "        # Infer√™ncia otimizada\n",
    "        response = llm(\n",
    "            prompt,\n",
    "            max_tokens=request.get(\"max_tokens\", 256),\n",
    "            temperature=request.get(\"temperature\", 0.7),\n",
    "            top_p=0.9,\n",
    "            stop=[\"\\\\n\\\\n\", \"User:\", \"Assistant:\"]\n",
    "        )\n",
    "        \n",
    "        return {\n",
    "            \"choices\": [{\n",
    "                \"message\": {\n",
    "                    \"role\": \"assistant\",\n",
    "                    \"content\": response[\"choices\"][0][\"text\"]\n",
    "                }\n",
    "            }],\n",
    "            \"model\": \"deepseek-gguf-cpu\",\n",
    "            \"backend\": \"llama.cpp-cpu\"\n",
    "        }\n",
    "        \n",
    "    except Exception as e:\n",
    "        raise HTTPException(status_code=500, detail=str(e))\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    uvicorn.run(app, host=\"0.0.0.0\", port=8000)\n",
    "'''\n",
    "    \n",
    "    return dockerfile_content, server_code\n",
    "\n",
    "# Configura√ß√£o de m√°quina CPU-otimizada\n",
    "def get_cpu_optimized_machine_spec():\n",
    "    \"\"\"Especifica√ß√£o de m√°quina otimizada para CPU.\"\"\"\n",
    "    \n",
    "    return {\n",
    "        \"machine_type\": \"c2-standard-8\",  # CPU otimizada\n",
    "        \"boot_disk_type\": \"pd-ssd\",\n",
    "        \"boot_disk_size_gb\": 50,\n",
    "        \"preemptible\": True,  # 70% economia\n",
    "        \"metadata\": {\n",
    "            \"items\": [\n",
    "                {\n",
    "                    \"key\": \"startup-script\",\n",
    "                    \"value\": '''#!/bin/bash\n",
    "                    # Script de otimiza√ß√£o CPU\n",
    "                    echo 'performance' | tee /sys/devices/system/cpu/cpu*/cpufreq/scaling_governor\n",
    "                    echo 1 > /proc/sys/vm/swappiness\n",
    "                    echo 3 > /proc/sys/vm/drop_caches\n",
    "                    '''\n",
    "                }\n",
    "            ]\n",
    "        }\n",
    "    }\n",
    "\n",
    "# Benchmark CPU vs GPU\n",
    "def benchmark_cpu_vs_gpu():\n",
    "    \"\"\"Compara√ß√£o de custos CPU vs GPU.\"\"\"\n",
    "    \n",
    "    costs = {\n",
    "        \"cpu_optimized\": {\n",
    "            \"machine\": \"c2-standard-8 (preemptible)\",\n",
    "            \"cost_hour_usd\": 0.096,  # Preemptible\n",
    "            \"cost_month_brl\": 0.096 * 24 * 30 * 5.2,  # ~360 BRL\n",
    "            \"performance\": \"Boa para infer√™ncia\",\n",
    "            \"pros\": [\"Muito mais barato\", \"Sem limita√ß√£o GPU\", \"GGML otimizado\"]\n",
    "        },\n",
    "        \"gpu_optimized\": {\n",
    "            \"machine\": \"n1-standard-4 + T4 (preemptible)\", \n",
    "            \"cost_hour_usd\": 0.35,   # Preemptible\n",
    "            \"cost_month_brl\": 0.35 * 24 * 30 * 5.2,  # ~1300 BRL\n",
    "            \"performance\": \"Excelente para infer√™ncia\",\n",
    "            \"pros\": [\"Mais r√°pido\", \"Melhor para modelos grandes\"]\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    print(\"üí∞ COMPARA√á√ÉO DE CUSTOS:\")\n",
    "    print(\"=\"*50)\n",
    "    \n",
    "    for config, details in costs.items():\n",
    "        print(f\"\\nüñ•Ô∏è {config.upper()}:\")\n",
    "        print(f\"   M√°quina: {details['machine']}\")\n",
    "        print(f\"   Custo/hora: ${details['cost_hour_usd']}\")\n",
    "        print(f\"   Custo/m√™s: R$ {details['cost_month_brl']:.0f}\")\n",
    "        print(f\"   Performance: {details['performance']}\")\n",
    "        print(f\"   Vantagens: {', '.join(details['pros'])}\")\n",
    "    \n",
    "    print(f\"\\nüí° RECOMENDA√á√ÉO:\")\n",
    "    print(f\"   Para economia m√°xima: Use CPU + GGML\")\n",
    "    print(f\"   Para performance: Use GPU s√≥ quando necess√°rio\")\n",
    "\n",
    "# Executar compara√ß√£o\n",
    "dockerfile, server = create_cpu_optimized_deployment()\n",
    "machine_spec = get_cpu_optimized_machine_spec()\n",
    "benchmark_cpu_vs_gpu()\n",
    "\n",
    "print(\"\\nüîß Arquivos gerados:\")\n",
    "print(\"- Dockerfile otimizado para CPU\")  \n",
    "print(\"- Servidor FastAPI com llama.cpp\")\n",
    "print(\"- Especifica√ß√£o de m√°quina econ√¥mica\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ace838be",
   "metadata": {},
   "source": [
    "## ‚ö° 7. Controle de Concorr√™ncia no FastAPI\n",
    "\n",
    "Limite requisi√ß√µes simult√¢neas para evitar satura√ß√£o e economizar recursos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85326bb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import asyncio\n",
    "from contextlib import asynccontextmanager\n",
    "from fastapi import FastAPI, HTTPException, BackgroundTasks\n",
    "import time\n",
    "\n",
    "class ResourceManager:\n",
    "    \"\"\"Gerenciador de recursos para controle de concorr√™ncia.\"\"\"\n",
    "    \n",
    "    def __init__(self, max_concurrent: int = 1, max_queue: int = 10):\n",
    "        self.semaphore = asyncio.Semaphore(max_concurrent)\n",
    "        self.queue_semaphore = asyncio.Semaphore(max_queue)\n",
    "        self.active_requests = 0\n",
    "        self.queued_requests = 0\n",
    "        self.total_requests = 0\n",
    "    \n",
    "    @asynccontextmanager\n",
    "    async def acquire_resource(self):\n",
    "        \"\"\"Context manager para controlar recursos.\"\"\"\n",
    "        \n",
    "        # Verificar se pode entrar na fila\n",
    "        async with self.queue_semaphore:\n",
    "            self.queued_requests += 1\n",
    "            \n",
    "            try:\n",
    "                # Aguardar recurso dispon√≠vel\n",
    "                async with self.semaphore:\n",
    "                    self.queued_requests -= 1\n",
    "                    self.active_requests += 1\n",
    "                    self.total_requests += 1\n",
    "                    \n",
    "                    print(f\"üöÄ Processando requisi√ß√£o (ativa: {self.active_requests})\")\n",
    "                    yield\n",
    "                    \n",
    "            finally:\n",
    "                self.active_requests -= 1\n",
    "                print(f\"‚úÖ Requisi√ß√£o conclu√≠da (ativa: {self.active_requests})\")\n",
    "    \n",
    "    def get_stats(self):\n",
    "        \"\"\"Estat√≠sticas do gerenciador.\"\"\"\n",
    "        return {\n",
    "            \"active\": self.active_requests,\n",
    "            \"queued\": self.queued_requests,\n",
    "            \"total_processed\": self.total_requests,\n",
    "            \"available_slots\": self.semaphore._value\n",
    "        }\n",
    "\n",
    "# Configura√ß√£o otimizada para economia\n",
    "resource_manager = ResourceManager(max_concurrent=1, max_queue=5)  # M√°ximo 1 simult√¢nea\n",
    "\n",
    "# FastAPI com controle de recursos\n",
    "app_optimized = FastAPI(title=\"DeepSeek Econ√≥mico\")\n",
    "\n",
    "@app_optimized.middleware(\"http\")\n",
    "async def resource_middleware(request, call_next):\n",
    "    \"\"\"Middleware para controle de recursos.\"\"\"\n",
    "    \n",
    "    start_time = time.time()\n",
    "    \n",
    "    # Verificar se h√° muitas requisi√ß√µes\n",
    "    stats = resource_manager.get_stats()\n",
    "    if stats[\"queued\"] >= 5:\n",
    "        return HTTPException(\n",
    "            status_code=429, \n",
    "            detail=\"Servidor ocupado. Tente novamente em alguns segundos.\"\n",
    "        )\n",
    "    \n",
    "    response = await call_next(request)\n",
    "    \n",
    "    process_time = time.time() - start_time\n",
    "    response.headers[\"X-Process-Time\"] = str(process_time)\n",
    "    response.headers[\"X-Active-Requests\"] = str(stats[\"active\"])\n",
    "    \n",
    "    return response\n",
    "\n",
    "@app_optimized.post(\"/v1/chat/completions\")\n",
    "async def economical_chat(request: dict, background_tasks: BackgroundTasks):\n",
    "    \"\"\"Endpoint com controle rigoroso de recursos.\"\"\"\n",
    "    \n",
    "    async with resource_manager.acquire_resource():\n",
    "        # Simular processamento\n",
    "        messages = request.get(\"messages\", [])\n",
    "        prompt = messages[-1][\"content\"] if messages else \"\"\n",
    "        \n",
    "        # Limitar tokens para economia\n",
    "        max_tokens = min(request.get(\"max_tokens\", 100), 200)  # M√°ximo 200 tokens\n",
    "        \n",
    "        # Aqui chamaria o modelo real (DeepSeek, Ollama, etc.)\n",
    "        await asyncio.sleep(2)  # Simular processamento\n",
    "        \n",
    "        # Resposta econ√¥mica\n",
    "        response = {\n",
    "            \"id\": f\"chatcmpl-{int(time.time())}\",\n",
    "            \"object\": \"chat.completion\",\n",
    "            \"choices\": [{\n",
    "                \"index\": 0,\n",
    "                \"message\": {\n",
    "                    \"role\": \"assistant\", \n",
    "                    \"content\": f\"Resposta econ√¥mica para: {prompt[:50]}...\"\n",
    "                },\n",
    "                \"finish_reason\": \"stop\"\n",
    "            }],\n",
    "            \"usage\": {\n",
    "                \"prompt_tokens\": len(prompt.split()),\n",
    "                \"completion_tokens\": max_tokens,\n",
    "                \"total_tokens\": len(prompt.split()) + max_tokens\n",
    "            },\n",
    "            \"model\": \"deepseek-economico\",\n",
    "            \"backend\": \"resource-controlled\"\n",
    "        }\n",
    "        \n",
    "        # Task em background para estat√≠sticas\n",
    "        background_tasks.add_task(log_usage, resource_manager.get_stats())\n",
    "        \n",
    "        return response\n",
    "\n",
    "@app_optimized.get(\"/health\")\n",
    "async def health_check():\n",
    "    \"\"\"Health check com informa√ß√µes de recursos.\"\"\"\n",
    "    stats = resource_manager.get_stats()\n",
    "    \n",
    "    return {\n",
    "        \"status\": \"healthy\",\n",
    "        \"resources\": stats,\n",
    "        \"memory_optimized\": True,\n",
    "        \"cost_mode\": \"economy\"\n",
    "    }\n",
    "\n",
    "async def log_usage(stats: dict):\n",
    "    \"\"\"Log de uso para monitoramento.\"\"\"\n",
    "    print(f\"üìä Stats: {stats}\")\n",
    "\n",
    "# Configura√ß√£o do servidor otimizada\n",
    "def create_economical_server_config():\n",
    "    \"\"\"Configura√ß√£o de servidor econ√¥mica.\"\"\"\n",
    "    \n",
    "    return {\n",
    "        \"host\": \"0.0.0.0\",\n",
    "        \"port\": 8000,\n",
    "        \"workers\": 1,              # Apenas 1 worker para economia\n",
    "        \"worker_class\": \"uvicorn.workers.UvicornWorker\",\n",
    "        \"worker_connections\": 10,   # Poucas conex√µes\n",
    "        \"max_requests\": 100,       # Reiniciar worker a cada 100 req\n",
    "        \"timeout\": 30,             # Timeout baixo\n",
    "        \"keepalive\": 2,            # Keep-alive baixo\n",
    "        \"preload\": True,           # Preload para economia mem√≥ria\n",
    "    }\n",
    "\n",
    "# Dockerfile otimizado para economia\n",
    "economical_dockerfile = '''\n",
    "FROM python:3.9-slim\n",
    "\n",
    "# Instalar apenas o essencial\n",
    "RUN pip install fastapi uvicorn[standard] --no-cache-dir\n",
    "\n",
    "# Copiar apenas arquivos necess√°rios\n",
    "COPY server.py /app/server.py\n",
    "WORKDIR /app\n",
    "\n",
    "# Configura√ß√µes de economia\n",
    "ENV PYTHONUNBUFFERED=1\n",
    "ENV PYTHONDONTWRITEBYTECODE=1\n",
    "ENV MAX_CONCURRENT=1\n",
    "ENV MAX_QUEUE=5\n",
    "\n",
    "# Limites de recursos\n",
    "ENV MALLOC_ARENA_MAX=2\n",
    "ENV PYTHONMALLOC=malloc\n",
    "\n",
    "EXPOSE 8000\n",
    "\n",
    "# Comando otimizado\n",
    "CMD [\"uvicorn\", \"server:app\", \"--host\", \"0.0.0.0\", \"--port\", \"8000\", \"--workers\", \"1\"]\n",
    "'''\n",
    "\n",
    "print(\"‚öôÔ∏è CONFIGURA√á√ÉO ECON√îMICA CRIADA:\")\n",
    "print(\"- M√°ximo 1 requisi√ß√£o simult√¢nea\")\n",
    "print(\"- Fila limitada a 5 requisi√ß√µes\")  \n",
    "print(\"- Timeout baixo (30s)\")\n",
    "print(\"- Tokens limitados (m√°x. 200)\")\n",
    "print(\"- Worker √∫nico para economia\")\n",
    "\n",
    "server_config = create_economical_server_config()\n",
    "print(f\"\\nüîß Config do servidor: {server_config}\")\n",
    "\n",
    "print(f\"\\nüí° ECONOMIA ESTIMADA:\")\n",
    "print(f\"- CPU: ~60% menos uso\")\n",
    "print(f\"- Mem√≥ria: ~40% menos uso\") \n",
    "print(f\"- Lat√™ncia: Controlada\")\n",
    "print(f\"- Custo: ~50% redu√ß√£o\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5eb1930",
   "metadata": {},
   "source": [
    "## üìä 8. Monitoramento de Recursos em Tempo Real\n",
    "\n",
    "Monitore CPU, GPU e mem√≥ria para otimizar custos continuamente."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21fa9648",
   "metadata": {},
   "outputs": [],
   "source": [
    "import psutil\n",
    "import GPUtil\n",
    "import threading\n",
    "import json\n",
    "from datetime import datetime\n",
    "\n",
    "class ResourceMonitor:\n",
    "    \"\"\"Monitor de recursos em tempo real.\"\"\"\n",
    "    \n",
    "    def __init__(self, alert_threshold: dict = None):\n",
    "        self.alert_threshold = alert_threshold or {\n",
    "            \"cpu_percent\": 80,\n",
    "            \"memory_percent\": 85,\n",
    "            \"gpu_percent\": 90\n",
    "        }\n",
    "        self.monitoring = False\n",
    "        self.stats_history = []\n",
    "    \n",
    "    def get_current_usage(self):\n",
    "        \"\"\"Obt√©m uso atual de recursos.\"\"\"\n",
    "        \n",
    "        # CPU e Mem√≥ria\n",
    "        cpu_percent = psutil.cpu_percent(interval=1)\n",
    "        memory = psutil.virtual_memory()\n",
    "        \n",
    "        stats = {\n",
    "            \"timestamp\": datetime.now().isoformat(),\n",
    "            \"cpu\": {\n",
    "                \"percent\": cpu_percent,\n",
    "                \"count\": psutil.cpu_count(),\n",
    "                \"freq_mhz\": psutil.cpu_freq().current if psutil.cpu_freq() else 0\n",
    "            },\n",
    "            \"memory\": {\n",
    "                \"percent\": memory.percent,\n",
    "                \"used_gb\": memory.used / (1024**3),\n",
    "                \"available_gb\": memory.available / (1024**3),\n",
    "                \"total_gb\": memory.total / (1024**3)\n",
    "            },\n",
    "            \"disk\": {\n",
    "                \"percent\": psutil.disk_usage('/').percent,\n",
    "                \"free_gb\": psutil.disk_usage('/').free / (1024**3)\n",
    "            }\n",
    "        }\n",
    "        \n",
    "        # GPU (se dispon√≠vel)\n",
    "        try:\n",
    "            gpus = GPUtil.getGPUs()\n",
    "            if gpus:\n",
    "                gpu = gpus[0]  # Primeira GPU\n",
    "                stats[\"gpu\"] = {\n",
    "                    \"percent\": gpu.load * 100,\n",
    "                    \"memory_used_gb\": gpu.memoryUsed / 1024,\n",
    "                    \"memory_total_gb\": gpu.memoryTotal / 1024,\n",
    "                    \"temperature\": gpu.temperature,\n",
    "                    \"name\": gpu.name\n",
    "                }\n",
    "        except:\n",
    "            stats[\"gpu\"] = None\n",
    "        \n",
    "        return stats\n",
    "    \n",
    "    def check_alerts(self, stats: dict):\n",
    "        \"\"\"Verifica alertas de recursos.\"\"\"\n",
    "        alerts = []\n",
    "        \n",
    "        # CPU Alert\n",
    "        if stats[\"cpu\"][\"percent\"] > self.alert_threshold[\"cpu_percent\"]:\n",
    "            alerts.append({\n",
    "                \"type\": \"CPU_HIGH\",\n",
    "                \"message\": f\"CPU em {stats['cpu']['percent']:.1f}%\",\n",
    "                \"action\": \"Considere reduzir concorr√™ncia\"\n",
    "            })\n",
    "        \n",
    "        # Memory Alert\n",
    "        if stats[\"memory\"][\"percent\"] > self.alert_threshold[\"memory_percent\"]:\n",
    "            alerts.append({\n",
    "                \"type\": \"MEMORY_HIGH\", \n",
    "                \"message\": f\"Mem√≥ria em {stats['memory']['percent']:.1f}%\",\n",
    "                \"action\": \"Limpe cache ou reduza batch size\"\n",
    "            })\n",
    "        \n",
    "        # GPU Alert\n",
    "        if stats.get(\"gpu\") and stats[\"gpu\"][\"percent\"] > self.alert_threshold[\"gpu_percent\"]:\n",
    "            alerts.append({\n",
    "                \"type\": \"GPU_HIGH\",\n",
    "                \"message\": f\"GPU em {stats['gpu']['percent']:.1f}%\",\n",
    "                \"action\": \"Otimize par√¢metros do modelo\"\n",
    "            })\n",
    "        \n",
    "        return alerts\n",
    "    \n",
    "    def start_monitoring(self, interval: int = 10):\n",
    "        \"\"\"Inicia monitoramento cont√≠nuo.\"\"\"\n",
    "        self.monitoring = True\n",
    "        \n",
    "        def monitor_loop():\n",
    "            while self.monitoring:\n",
    "                try:\n",
    "                    stats = self.get_current_usage()\n",
    "                    alerts = self.check_alerts(stats)\n",
    "                    \n",
    "                    # Adicionar ao hist√≥rico\n",
    "                    self.stats_history.append(stats)\n",
    "                    \n",
    "                    # Manter apenas √∫ltimas 100 medi√ß√µes\n",
    "                    if len(self.stats_history) > 100:\n",
    "                        self.stats_history.pop(0)\n",
    "                    \n",
    "                    # Exibir alertas\n",
    "                    if alerts:\n",
    "                        print(f\"üö® ALERTAS DE RECURSOS:\")\n",
    "                        for alert in alerts:\n",
    "                            print(f\"   {alert['type']}: {alert['message']}\")\n",
    "                            print(f\"   A√ß√£o: {alert['action']}\")\n",
    "                    \n",
    "                    time.sleep(interval)\n",
    "                    \n",
    "                except Exception as e:\n",
    "                    print(f\"‚ùå Erro no monitoramento: {e}\")\n",
    "                    time.sleep(interval)\n",
    "        \n",
    "        # Iniciar thread de monitoramento\n",
    "        monitor_thread = threading.Thread(target=monitor_loop, daemon=True)\n",
    "        monitor_thread.start()\n",
    "        \n",
    "        print(f\"üìä Monitoramento iniciado (intervalo: {interval}s)\")\n",
    "    \n",
    "    def stop_monitoring(self):\n",
    "        \"\"\"Para monitoramento.\"\"\"\n",
    "        self.monitoring = False\n",
    "        print(\"‚èπÔ∏è Monitoramento parado\")\n",
    "    \n",
    "    def get_optimization_recommendations(self):\n",
    "        \"\"\"Recomenda√ß√µes baseadas no hist√≥rico.\"\"\"\n",
    "        if not self.stats_history:\n",
    "            return [\"Sem dados hist√≥ricos dispon√≠veis\"]\n",
    "        \n",
    "        # An√°lise dos √∫ltimos 10 registros\n",
    "        recent_stats = self.stats_history[-10:]\n",
    "        \n",
    "        avg_cpu = sum(s[\"cpu\"][\"percent\"] for s in recent_stats) / len(recent_stats)\n",
    "        avg_memory = sum(s[\"memory\"][\"percent\"] for s in recent_stats) / len(recent_stats)\n",
    "        \n",
    "        recommendations = []\n",
    "        \n",
    "        # Recomenda√ß√µes baseadas em uso\n",
    "        if avg_cpu < 30:\n",
    "            recommendations.append(\"üí° CPU subutilizada - considere inst√¢ncia menor\")\n",
    "            recommendations.append(\"üí∞ Economia potencial: 30-50% do custo\")\n",
    "        \n",
    "        if avg_memory < 50:\n",
    "            recommendations.append(\"üí° Mem√≥ria subutilizada - reduza configura√ß√£o\")\n",
    "            recommendations.append(\"üí∞ Economia potencial: 20-40% do custo\")\n",
    "        \n",
    "        if avg_cpu > 80:\n",
    "            recommendations.append(\"‚ö†Ô∏è CPU sobrecarregada - limite concorr√™ncia\")\n",
    "            recommendations.append(\"üîß Configure max_concurrent=1\")\n",
    "        \n",
    "        if avg_memory > 85:\n",
    "            recommendations.append(\"‚ö†Ô∏è Mem√≥ria alta - implemente cleanup\")\n",
    "            recommendations.append(\"üîß Ative garbage collection agressivo\")\n",
    "        \n",
    "        return recommendations\n",
    "\n",
    "# Configurar monitoramento econ√¥mico\n",
    "monitor = ResourceMonitor(alert_threshold={\n",
    "    \"cpu_percent\": 70,    # Alertar mais cedo para economia\n",
    "    \"memory_percent\": 80,  \n",
    "    \"gpu_percent\": 85\n",
    "})\n",
    "\n",
    "# Estat√≠sticas atuais\n",
    "current_stats = monitor.get_current_usage()\n",
    "print(\"üìä ESTAT√çSTICAS ATUAIS:\")\n",
    "print(f\"CPU: {current_stats['cpu']['percent']:.1f}%\")\n",
    "print(f\"Mem√≥ria: {current_stats['memory']['percent']:.1f}% ({current_stats['memory']['used_gb']:.1f}GB)\")\n",
    "print(f\"Disco: {current_stats['disk']['percent']:.1f}%\")\n",
    "\n",
    "if current_stats.get(\"gpu\"):\n",
    "    print(f\"GPU: {current_stats['gpu']['percent']:.1f}% ({current_stats['gpu']['name']})\")\n",
    "else:\n",
    "    print(\"GPU: N√£o detectada (modo CPU)\")\n",
    "\n",
    "# Script de otimiza√ß√£o autom√°tica\n",
    "def create_auto_optimizer():\n",
    "    \"\"\"Cria script de otimiza√ß√£o autom√°tica.\"\"\"\n",
    "    \n",
    "    optimizer_script = '''#!/bin/bash\n",
    "# Script de otimiza√ß√£o autom√°tica para economia\n",
    "\n",
    "# Fun√ß√£o para otimizar CPU\n",
    "optimize_cpu() {\n",
    "    echo \"üîß Otimizando CPU...\"\n",
    "    \n",
    "    # Configurar governor para economia\n",
    "    echo 'powersave' | tee /sys/devices/system/cpu/cpu*/cpufreq/scaling_governor\n",
    "    \n",
    "    # Reduzir threads se CPU baixo\n",
    "    CPU_USAGE=$(top -bn1 | grep \"Cpu(s)\" | awk '{print $2}' | sed 's/%us,//')\n",
    "    if (( $(echo \"$CPU_USAGE < 30\" | bc -l) )); then\n",
    "        echo \"üí° CPU subutilizada, reduzindo threads\"\n",
    "        export OMP_NUM_THREADS=2\n",
    "    fi\n",
    "}\n",
    "\n",
    "# Fun√ß√£o para otimizar mem√≥ria  \n",
    "optimize_memory() {\n",
    "    echo \"üîß Otimizando mem√≥ria...\"\n",
    "    \n",
    "    # Limpeza agressiva de cache\n",
    "    sync && echo 3 > /proc/sys/vm/drop_caches\n",
    "    \n",
    "    # Configurar swap\n",
    "    echo 10 > /proc/sys/vm/swappiness\n",
    "    \n",
    "    # Garbage collection Python\n",
    "    python3 -c \"import gc; gc.collect()\"\n",
    "}\n",
    "\n",
    "# Executar otimiza√ß√µes\n",
    "optimize_cpu\n",
    "optimize_memory\n",
    "\n",
    "echo \"‚úÖ Otimiza√ß√£o conclu√≠da\"\n",
    "'''\n",
    "    \n",
    "    return optimizer_script\n",
    "\n",
    "optimizer = create_auto_optimizer()\n",
    "print(f\"\\nüîß Script de otimiza√ß√£o criado\")\n",
    "print(f\"üí° Execute periodicamente para manter economia\")\n",
    "\n",
    "# Iniciar monitoramento (descomente para usar)\n",
    "# monitor.start_monitoring(interval=30)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae18d223",
   "metadata": {},
   "source": [
    "## üéØ Resumo e Plano de A√ß√£o\n",
    "\n",
    "### üí∞ **Meta de Economia: Reduzir custos para < R$ 300/m√™s**\n",
    "\n",
    "| Estrat√©gia | Economia | Implementa√ß√£o | Prioridade |\n",
    "|------------|----------|---------------|------------|\n",
    "| Spot Instances | 70% | Imediata | üî• Alta |\n",
    "| CPU + GGML | 60% | 1-2 dias | üî• Alta |\n",
    "| Auto-Stop | 40% | Imediata | üü° M√©dia |\n",
    "| Concorr√™ncia=1 | 50% | Imediata | üü° M√©dia |\n",
    "| Cache Local | 20% | 1 dia | üü¢ Baixa |\n",
    "\n",
    "### üìã **Checklist de Implementa√ß√£o:**\n",
    "\n",
    "#### ‚úÖ **Implementar Imediatamente:**\n",
    "- [ ] Ativar spot instances em todas as VMs\n",
    "- [ ] Configurar auto-stop √†s 22:00\n",
    "- [ ] Limitar concorr√™ncia para 1 requisi√ß√£o\n",
    "- [ ] Configurar alertas de or√ßamento\n",
    "\n",
    "#### üîß **Implementar em 1-2 dias:**\n",
    "- [ ] Migrar para CPU + GGML\n",
    "- [ ] Configurar cache persistente de modelos\n",
    "- [ ] Implementar monitoramento de recursos\n",
    "- [ ] Otimizar Dockerfile para economia\n",
    "\n",
    "#### üìä **Monitorar Continuamente:**\n",
    "- [ ] Dashboard de custos di√°rio\n",
    "- [ ] Alertas de CPU/Mem√≥ria\n",
    "- [ ] Performance vs custo\n",
    "- [ ] Usage patterns\n",
    "\n",
    "### üö® **Comandos de Emerg√™ncia (se custo alto):**\n",
    "\n",
    "```bash\n",
    "# Parar todas as inst√¢ncias\n",
    "gcloud compute instances stop --all --zone=us-central1-a\n",
    "\n",
    "# Verificar custos atuais\n",
    "gcloud billing budgets list\n",
    "\n",
    "# Ativar s√≥ DeepSeek 1.3B\n",
    "kubectl scale deployment ollama-deepseek --replicas=0\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
